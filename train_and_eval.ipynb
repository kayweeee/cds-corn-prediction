{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import pickle\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from models import CNN_LSTM, SepCNN_LSTM, ConvGRU_LSTM, RandomForestBaseline, LassoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2506c0ac450>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "# Set seed for NumPy\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set seed for Python's built-in random module\n",
    "random.seed(seed)\n",
    "\n",
    "# Set seed for PyTorch\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Set seed for Torch's CUDA operations if GPU is used\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/PROCESSEDIII/2021_51_101.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# # Load a sample of the data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sample_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/PROCESSEDIII/2021_51_101.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# # Check the shape of the sample data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of sample data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_data\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\sds\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/PROCESSEDIII/2021_51_101.npy'"
     ]
    }
   ],
   "source": [
    "# # Load a sample of the data\n",
    "sample_data = np.load('./data/PROCESSEDIII/2021_51_101.npy')  \n",
    "\n",
    "# # Check the shape of the sample data\n",
    "print(\"Shape of sample data:\", sample_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generator function\n",
    "def generator(IDs, yields, batch_size, cutoff=None):\n",
    "    def load_data(ID):\n",
    "        try:\n",
    "            data = np.load('./data/PROCESSED_III/' + ID + '.npy')\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            # print('Error loading data:', e)\n",
    "            return None\n",
    "\n",
    "    batches = 0\n",
    "\n",
    "    while True:\n",
    "        batch_features = np.zeros((batch_size, 38, 1, 128, 9)) if cutoff is None else np.zeros((batch_size, cutoff, 1, 128, 9))\n",
    "        batch_yields = np.zeros(batch_size)\n",
    "\n",
    "        if batches == len(IDs) // batch_size:\n",
    "            batches = 0\n",
    "            yield None, None\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            index = random.choice(range(len(IDs)))\n",
    "            ID = IDs[index]\n",
    "            data = load_data(ID)\n",
    "\n",
    "            if data is not None:\n",
    "                if cutoff is not None:\n",
    "                    if not np.isnan(data).any():\n",
    "                        batch_features[i, :, :, :, :] = data[:cutoff, :, :, :]\n",
    "                        batch_yields[i] = yields[ID]\n",
    "                    else:\n",
    "                        print('Data contains NaN values:', ID)\n",
    "                else:\n",
    "                    batch_features[i, :, :, :, :] = data\n",
    "                    batch_yields[i] = yields[ID]\n",
    "\n",
    "        batches += 1\n",
    "\n",
    "        yield torch.tensor(batch_features, dtype=torch.float32, device='cuda'), torch.tensor(batch_yields, dtype=torch.float32, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "yields = pickle.load(open('data/yields.p', 'rb'))\n",
    "\n",
    "# Generators\n",
    "training_generator = generator(list(yields['train'].keys()), yields['train'], 16)\n",
    "validation_generator = generator(list(yields['validation'].keys()), yields['validation'], 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1891.901342797789\n"
     ]
    }
   ],
   "source": [
    "## for Random Forest baseline model\n",
    "random_forest_model = RandomForestBaseline(n_estimators=100, max_depth=None, random_state=42)\n",
    "\n",
    "# Fit the model to training data\n",
    "X_train, y_train = next(training_generator)\n",
    "random_forest_model.fit(X_train.cpu().reshape(X_train.shape[0], -1), y_train.cpu())\n",
    "\n",
    "# Make predictions on test data\n",
    "X_test, y_test = next(validation_generator)\n",
    "predictions = random_forest_model.predict(X_test.cpu().reshape(X_test.shape[0], -1))  # Flattening input features\n",
    "\n",
    "# Evaluate the model\n",
    "mse = random_forest_model.evaluate(X_test.cpu().reshape(X_test.cpu().shape[0], -1), y_test.cpu())  # Flattening input features\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3469.941841439345\n"
     ]
    }
   ],
   "source": [
    "## for Lasso baseline model\n",
    "lasso_model = LassoModel(alpha=0.5, random_state=42)\n",
    "\n",
    "# Fit the model to training data\n",
    "X_train, y_train = next(training_generator)\n",
    "lasso_model.fit(X_train.cpu().reshape(X_train.shape[0], -1), y_train.cpu())\n",
    "\n",
    "# Make predictions on test data\n",
    "X_test, y_test = next(validation_generator)\n",
    "predictions = lasso_model.predict(X_test.cpu().reshape(X_test.shape[0], -1))  # Flattening input features\n",
    "\n",
    "# Evaluate the model\n",
    "mse = lasso_model.evaluate(X_test.cpu().reshape(X_test.cpu().shape[0], -1), y_test.cpu())  # Flattening input features\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 409it [00:25, 16.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save best model\n",
      "Epoch 1/10, Train Loss: 4309.9201, Val Loss: 3655.0766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 409it [00:25, 16.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save best model\n",
      "Epoch 2/10, Train Loss: 1380.0852, Val Loss: 651.1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 409it [00:25, 16.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save best model\n",
      "Epoch 3/10, Train Loss: 772.5979, Val Loss: 439.3886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 61it [00:03, 15.91it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     27\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 28\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     31\u001b[0m val_losses \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_functions = {\n",
    "    'CNN_LSTM': CNN_LSTM,\n",
    "    # 'SepCNN_LSTM': SepCNN_LSTM,\n",
    "    # 'ConvGRU_LSTM': ConvGRU_LSTM,\n",
    "}\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for model_name, model_function in model_functions.items():\n",
    "    model = model_function(dimensions=[38, 1, 128, 9])\n",
    "    model.to('cuda')\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch_data, batch_labels in tqdm.tqdm(training_generator, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            if batch_data is None:\n",
    "                break\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        best_loss = float('inf')\n",
    "        with torch.no_grad():\n",
    "            for val_data, val_labels in validation_generator:\n",
    "                if val_data is None:\n",
    "                    break\n",
    "                val_outputs = model(val_data)\n",
    "                val_loss = criterion(val_outputs, val_labels.unsqueeze(1))\n",
    "                val_losses.append(val_loss.item())\n",
    "        current_loss = np.mean(val_losses)\n",
    "        if current_loss < best_loss:\n",
    "            print('save best model')\n",
    "            torch.save(model, f'{model_name}_best.pt')\n",
    "            best_loss = current_loss\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {np.mean(train_losses):.4f}, Val Loss: {current_loss:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model, f'{model_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
